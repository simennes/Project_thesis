{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1e3301b",
   "metadata": {},
   "source": [
    "## Configurable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63becf35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c0d9001",
   "metadata": {},
   "source": [
    "## Build graph (adjacency matrix) from GRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc51a0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadr\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def grm_to_adjacency(GRM_df, k=3, weighted=False):\n",
    "    \"\"\"\n",
    "    Build adjacency matrix (sparse) from GRM using kNN.\n",
    "    \n",
    "    GRM_df: pd.DataFrame, index and columns are IDs\n",
    "    k: int, number of neighbors\n",
    "    weighted: bool, if True use GRM similarity as weights, else binary\n",
    "\n",
    "    returns: scipy sparse adjacency (CSR)\n",
    "    \"\"\"\n",
    "    # --- Load GRM ---\n",
    "    ids = GRM_df.index.to_numpy().astype(str)\n",
    "    G = GRM_df.to_numpy().astype(float)\n",
    "\n",
    "    # --- Normalize (cosine-like) ---\n",
    "    diag = np.clip(np.diag(G), 1e-12, None)\n",
    "    D = np.sqrt(np.outer(diag, diag))\n",
    "    G_norm = G / D\n",
    "    G_norm = np.clip(G_norm, -1.0, 1.0)\n",
    "\n",
    "    # --- kNN search ---\n",
    "    dist = 1.0 - G_norm   # similarity â†’ distance\n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1, metric=\"precomputed\").fit(dist)\n",
    "    dists, neigh = nbrs.kneighbors(dist)\n",
    "\n",
    "    rows, cols, data = [], [], []\n",
    "    for i in range(len(ids)):\n",
    "        for j in neigh[i][1:]:  # skip self\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "            if weighted:\n",
    "                data.append(G_norm[i, j])\n",
    "            else:\n",
    "                data.append(1.0)\n",
    "\n",
    "    A = sp.csr_matrix((data, (rows, cols)), shape=(len(ids), len(ids)))\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b38e314",
   "metadata": {},
   "source": [
    "## Figure (for fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d21c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def plot_grm_graph(grm_file, pheno_file, subset_size=100, k=5, seed=42):\n",
    "    \"\"\"\n",
    "    Plot kNN graph from GRM, coloring nodes by adjusted phenotype.\n",
    "\n",
    "    grm_file: str, path to GRM RDS file\n",
    "    pheno_file: str, CSV with columns [\"ringnr\", \"y_adjusted\"]\n",
    "    subset_size: int, number of individuals to plot\n",
    "    k: int, nearest neighbors\n",
    "    \"\"\"\n",
    "    # --- Load GRM ---\n",
    "    grm_res = pyreadr.read_r(grm_file)\n",
    "    GRM_df = grm_res[None]\n",
    "    ids = GRM_df.index.to_numpy().astype(str)\n",
    "    G = GRM_df.to_numpy().astype(float)\n",
    "\n",
    "    # --- Load phenotypes ---\n",
    "    pheno = pd.read_csv(pheno_file)\n",
    "    pheno = pheno.set_index(\"ringnr\").loc[ids]  # align order\n",
    "    y = pheno[\"y_adjusted\"].to_numpy()\n",
    "\n",
    "    # --- Subset ---\n",
    "    np.random.seed(seed)\n",
    "    subset = np.random.choice(len(ids), min(subset_size, len(ids)), replace=False)\n",
    "    G = G[np.ix_(subset, subset)]\n",
    "    ids = ids[subset]\n",
    "    y = y[subset]\n",
    "\n",
    "    # --- Normalize GRM (cosine-like) ---\n",
    "    diag = np.clip(np.diag(G), 1e-12, None)\n",
    "    D = np.sqrt(np.outer(diag, diag))\n",
    "    G_norm = G / D\n",
    "    G_norm = np.clip(G_norm, -1.0, 1.0)\n",
    "\n",
    "    # --- Build kNN graph ---\n",
    "    dist = 1.0 - G_norm\n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1, metric=\"precomputed\").fit(dist)\n",
    "    dists, neigh = nbrs.kneighbors(dist)\n",
    "\n",
    "    edges = []\n",
    "    for i in range(len(ids)):\n",
    "        for j in neigh[i][1:]:   # skip self\n",
    "            edges.append((i, j))\n",
    "\n",
    "    # --- Build NetworkX graph ---\n",
    "    G_nx = nx.Graph()\n",
    "    for i, node in enumerate(ids):\n",
    "        G_nx.add_node(i, ringnr=node, phenotype=y[i])\n",
    "    G_nx.add_edges_from(edges)\n",
    "\n",
    "    # --- Plot ---\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    pos = nx.spring_layout(G_nx, seed=seed)\n",
    "\n",
    "    # color by phenotype\n",
    "    node_colors = [d[\"phenotype\"] for _, d in G_nx.nodes(data=True)]\n",
    "    nodes = nx.draw_networkx_nodes(G_nx, pos,\n",
    "                                   node_size=80,\n",
    "                                   node_color=node_colors,\n",
    "                                   cmap=plt.cm.viridis)\n",
    "    nx.draw_networkx_edges(G_nx, pos, alpha=0.3)\n",
    "\n",
    "    plt.colorbar(nodes, label=\"Adjusted phenotype\")\n",
    "    plt.title(f\"kNN graph (subset={len(ids)}, k={k})\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "plot_grm_graph(\n",
    "    grm_file=\"Data/gnn/GRM_vanraden_1000.rds\",\n",
    "    pheno_file=\"Data/gnn/adjusted_body_mass.csv\",\n",
    "    subset_size=100,\n",
    "    k=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d360eb6e",
   "metadata": {},
   "source": [
    "## Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aeac424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def add_self_loops_csr(A: sp.csr_matrix):\n",
    "    n = A.shape[0]\n",
    "    A = A.tolil()\n",
    "    A.setdiag(1)                      # add I\n",
    "    return A.tocsr()\n",
    "\n",
    "def row_normalize_csr(A: sp.csr_matrix):\n",
    "    # \\hat{A} = D_row^{-1} A, where D_row[i,i] = sum_j A[i,j]\n",
    "    d = torch.from_numpy(np.asarray(A.sum(axis=1)).ravel()).float()\n",
    "    d[d == 0] = 1.0\n",
    "    inv = 1.0 / d\n",
    "    Dinv = sp.diags(inv.numpy())\n",
    "    return Dinv.dot(A).tocsr()\n",
    "\n",
    "def csr_to_torch_sparse(A: sp.csr_matrix):\n",
    "    A = A.tocoo()\n",
    "    indices = torch.tensor([A.row, A.col], dtype=torch.long)\n",
    "    values = torch.tensor(A.data, dtype=torch.float32)\n",
    "    return torch.sparse_coo_tensor(indices, values, torch.Size(A.shape)).coalesce()\n",
    "\n",
    "class GCN_RS_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    A configurable GCN-style regressor:\n",
    "      - Each hidden layer: H^{(l+1)} = Dropout( ReLU( BN( Linear( A @ H^{(l)} ) ) ) )\n",
    "      - Final head: out = Linear(H^{(L)})\n",
    "    Args\n",
    "    ----\n",
    "    input_dim: int\n",
    "        Number of SNP features per individual.\n",
    "    hidden_dims: list[int]\n",
    "        Sizes of hidden layers (e.g., [128, 64]). Length = #graph conv layers.\n",
    "    dropout: float\n",
    "        Dropout probability applied after each hidden layer.\n",
    "    use_batchnorm: bool\n",
    "        If True, apply BatchNorm1d after Linear and before ReLU.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 hidden_dims=(128, 64),\n",
    "                 dropout: float = 0.5,\n",
    "                 use_batchnorm: bool = True):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.use_bn = use_batchnorm\n",
    "\n",
    "        # Define dimensions from input through hidden layers\n",
    "        dims = [input_dim] + list(hidden_dims)\n",
    "\n",
    "        # Linear maps after graph aggregation\n",
    "        self.linears = nn.ModuleList([\n",
    "            nn.Linear(dims[i], dims[i+1], bias=True)\n",
    "            for i in range(len(dims) - 1)\n",
    "        ])\n",
    "\n",
    "        # Optional BN per hidden layer\n",
    "        if self.use_bn:\n",
    "            self.bns = nn.ModuleList([nn.BatchNorm1d(d) for d in hidden_dims])\n",
    "        else:\n",
    "            self.bns = None\n",
    "\n",
    "        # Final regression head\n",
    "        self.head = nn.Linear(dims[-1], 1)\n",
    "\n",
    "    def forward(self, X: torch.Tensor, A_norm: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        X:       [N, F] dense features\n",
    "        A_norm:  [N, N] torch.sparse_coo_tensor (row-normalized with self-loops)\n",
    "        \"\"\"\n",
    "        H = X\n",
    "        for i, lin in enumerate(self.linears):\n",
    "            # Graph message passing\n",
    "            H = torch.sparse.mm(A_norm, H)   # [N, d_in]\n",
    "            H = lin(H)                       # [N, d_out]\n",
    "            if self.use_bn:\n",
    "                H = self.bns[i](H)           # BN on features\n",
    "            H = F.relu(H)\n",
    "            H = F.dropout(H, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Final linear head (no propagation here)\n",
    "        out = self.head(H).squeeze(-1)       # [N]\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64c025f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_train_test(X, train_idx, test_idx):\n",
    "    X_train = X[train_idx]\n",
    "    mean = X_train.mean(0, keepdim=True)\n",
    "    std = X_train.std(0, keepdim=True).clamp_min(1e-6)\n",
    "    Xs = (X - mean) / std\n",
    "    return Xs, mean, std\n",
    "\n",
    "def standardize_y_train_test(y, train_idx, test_idx):\n",
    "    y_train = y[train_idx]\n",
    "    m = y_train.mean()\n",
    "    s = y_train.std().clamp_min(1e-6)\n",
    "    ys = (y - m) / s\n",
    "    return ys, m, s\n",
    "\n",
    "def subgraph_norm_to_sparse(A_full_csr: sp.csr_matrix, node_idx: np.ndarray) -> torch.Tensor:\n",
    "    # Induce subgraph, then add self-loops, row-normalize, convert to torch.sparse\n",
    "    A_sub = A_full_csr[node_idx][:, node_idx]\n",
    "    A_sub = add_self_loops_csr(A_sub)\n",
    "    A_sub = row_normalize_csr(A_sub)\n",
    "    return csr_to_torch_sparse(A_sub)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b66f1a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41d1c3b",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc6b576c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After intersection:\n",
      " SNP shape: (5717, 65344)\n",
      " Pheno shape: (5717, 3)\n",
      " GRM shape: 5717\n",
      " Any NaNs in SNP? False\n",
      " Any NaNs in Pheno? False\n",
      "-----------------------------\n",
      "Adjacency: (5717, 5717) edges: 28585\n",
      "Features: torch.Size([5717, 65344])\n",
      "Phenotypes: torch.Size([5717])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ---------- SNP FEATURES ----------\n",
    "snp_file = \"../Data/gnn/SNP/ALL/snp_export_body_mass_ALL_geno.feather\"\n",
    "snp_df = pd.read_feather(snp_file)  # expects first column 'ringnr'\n",
    "\n",
    "# ---------- PHENOTYPE ----------\n",
    "pheno_file = \"../Data/gnn/adjusted_body_mass.csv\"\n",
    "pheno_df = pd.read_csv(pheno_file)\n",
    "\n",
    "# ---------- GRM ----------\n",
    "grm_file = \"../Data/gnn/GRM/GRM_vanraden_PLINK.rds\"\n",
    "grm_res = pyreadr.read_r(grm_file)\n",
    "GRM_df = grm_res[None]  # pandas DataFrame with row/col names\n",
    "GRM_df.index = GRM_df.index.astype(str)\n",
    "GRM_df.columns = GRM_df.columns.astype(str)\n",
    "ids_grm = GRM_df.index.values  # already str\n",
    "\n",
    "# ---------- ALIGN EVERYTHING ----------\n",
    "# cast IDs to str\n",
    "snp_df[\"ringnr\"] = snp_df[\"ringnr\"].astype(str)\n",
    "pheno_df[\"ringnr\"] = pheno_df[\"ringnr\"].astype(str)\n",
    "\n",
    "# set indices\n",
    "snp_df = snp_df.set_index(\"ringnr\")\n",
    "pheno_df = pheno_df.set_index(\"ringnr\")\n",
    "\n",
    "# intersection in GRM order (preserve GRM row/col ordering)\n",
    "mask_in = np.isin(ids_grm, snp_df.index.values) & np.isin(ids_grm, pheno_df.index.values)\n",
    "ids_common = ids_grm[mask_in]  # preserves GRM order\n",
    "\n",
    "# reindex SNP + pheno to this exact order (no NaNs should remain)\n",
    "snp_df = snp_df.reindex(ids_common)\n",
    "pheno_df = pheno_df.reindex(ids_common)\n",
    "\n",
    "# slice GRM to the same IDs/order\n",
    "GRM_sub = GRM_df.loc[ids_common, ids_common]\n",
    "\n",
    "# build adjacency from sliced GRM (your function must accept a DataFrame)\n",
    "A_knn_csr = grm_to_adjacency(GRM_sub, k=5, weighted=False)\n",
    "\n",
    "# ---------- TENSORS ----------\n",
    "X_features = torch.tensor(snp_df.values, dtype=torch.float32)\n",
    "y_values = torch.tensor(pheno_df[\"y_adjusted\"].values, dtype=torch.float32)\n",
    "\n",
    "# ---------- SANITY CHECK ----------\n",
    "print(\"After intersection:\")\n",
    "print(\" SNP shape:\", snp_df.shape)\n",
    "print(\" Pheno shape:\", pheno_df.shape)\n",
    "print(\" GRM shape:\", GRM_sub.shape[0])\n",
    "print(\" Any NaNs in SNP?\", snp_df.isna().any().any())\n",
    "print(\" Any NaNs in Pheno?\", pheno_df.isna().any().any())\n",
    "print(\"-----------------------------\")\n",
    "print(\"Adjacency:\", A_knn_csr.shape, \"edges:\", A_knn_csr.nnz)\n",
    "print(\"Features:\", X_features.shape)\n",
    "print(\"Phenotypes:\", y_values.shape)\n",
    "\n",
    "assert snp_df.shape[0] == pheno_df.shape[0] == GRM_sub.shape[0] == A_knn_csr.shape[0], \"Shapes must match\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abb8ec97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>5707</th>\n",
       "      <th>5708</th>\n",
       "      <th>5709</th>\n",
       "      <th>5710</th>\n",
       "      <th>5711</th>\n",
       "      <th>5712</th>\n",
       "      <th>5713</th>\n",
       "      <th>5714</th>\n",
       "      <th>5715</th>\n",
       "      <th>5716</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5712</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5713</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5714</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5715</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5716</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5717 rows Ã— 5717 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4     5     6     7     \\\n",
       "0     0.166667  0.166667  0.000000  0.000000  0.000000   0.0   0.0   0.0   \n",
       "1     0.166667  0.166667  0.000000  0.000000  0.000000   0.0   0.0   0.0   \n",
       "2     0.000000  0.000000  0.166667  0.000000  0.000000   0.0   0.0   0.0   \n",
       "3     0.000000  0.000000  0.000000  0.166667  0.000000   0.0   0.0   0.0   \n",
       "4     0.000000  0.000000  0.000000  0.000000  0.166667   0.0   0.0   0.0   \n",
       "...        ...       ...       ...       ...       ...   ...   ...   ...   \n",
       "5712  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   0.0   0.0   \n",
       "5713  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   0.0   0.0   \n",
       "5714  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   0.0   0.0   \n",
       "5715  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   0.0   0.0   \n",
       "5716  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   0.0   0.0   \n",
       "\n",
       "      8     9     ...  5707  5708  5709      5710      5711      5712  \\\n",
       "0      0.0   0.0  ...   0.0   0.0   0.0  0.000000  0.000000  0.000000   \n",
       "1      0.0   0.0  ...   0.0   0.0   0.0  0.000000  0.000000  0.000000   \n",
       "2      0.0   0.0  ...   0.0   0.0   0.0  0.166667  0.000000  0.000000   \n",
       "3      0.0   0.0  ...   0.0   0.0   0.0  0.000000  0.000000  0.000000   \n",
       "4      0.0   0.0  ...   0.0   0.0   0.0  0.000000  0.000000  0.000000   \n",
       "...    ...   ...  ...   ...   ...   ...       ...       ...       ...   \n",
       "5712   0.0   0.0  ...   0.0   0.0   0.0  0.000000  0.000000  0.166667   \n",
       "5713   0.0   0.0  ...   0.0   0.0   0.0  0.000000  0.166667  0.166667   \n",
       "5714   0.0   0.0  ...   0.0   0.0   0.0  0.000000  0.000000  0.000000   \n",
       "5715   0.0   0.0  ...   0.0   0.0   0.0  0.000000  0.000000  0.000000   \n",
       "5716   0.0   0.0  ...   0.0   0.0   0.0  0.000000  0.000000  0.000000   \n",
       "\n",
       "          5713      5714      5715      5716  \n",
       "0     0.000000  0.000000  0.000000  0.000000  \n",
       "1     0.000000  0.000000  0.000000  0.000000  \n",
       "2     0.000000  0.000000  0.000000  0.000000  \n",
       "3     0.000000  0.000000  0.000000  0.000000  \n",
       "4     0.000000  0.000000  0.000000  0.000000  \n",
       "...        ...       ...       ...       ...  \n",
       "5712  0.166667  0.000000  0.000000  0.000000  \n",
       "5713  0.166667  0.000000  0.000000  0.000000  \n",
       "5714  0.000000  0.166667  0.000000  0.000000  \n",
       "5715  0.000000  0.000000  0.166667  0.000000  \n",
       "5716  0.000000  0.000000  0.000000  0.166667  \n",
       "\n",
       "[5717 rows x 5717 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describe A_knn_csr\n",
    "A_norm = subgraph_norm_to_sparse(A_knn_csr, np.arange(A_knn_csr.shape[0]))\n",
    "a_df = pd.DataFrame(np.array(A_norm.to_dense()))\n",
    "a_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "500f7ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIMS   = [128, 64]   # e.g., [64] or [256,128,64]\n",
    "DROPOUT       = 0.4\n",
    "USE_BATCHNORM = True\n",
    "PCA_DIM = False\n",
    "\n",
    "subgraph_count = 5      # number of random subgraphs (paper uses multiple)\n",
    "subgraph_frac  = 0.8    # fraction of train nodes per subgraph\n",
    "val_frac       = 0.1    # 10% validation inside each subgraph\n",
    "max_epochs     = 200\n",
    "patience       = 20\n",
    "lr             = 1e-4\n",
    "weight_decay   = 5e-4   # light L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca8bfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simen\\AppData\\Local\\Temp\\ipykernel_13908\\239036980.py:21: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:256.)\n",
      "  indices = torch.tensor([A.row, A.col], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 1 | Subgraph 1 | Epoch 001 | Train Loss 1.3562 | Val Loss 0.7422\n",
      "  Fold 1 | Subgraph 1 | Epoch 010 | Train Loss 0.6982 | Val Loss 0.7595\n",
      "  Fold 1 | Subgraph 1 | Epoch 020 | Train Loss 0.5728 | Val Loss 0.8441\n",
      "  Early stopping at epoch 25 (best val=0.7191)\n",
      "  Fold 1 | Subgraph 2 | Epoch 001 | Train Loss 1.2580 | Val Loss 0.9170\n",
      "  Fold 1 | Subgraph 2 | Epoch 010 | Train Loss 0.6155 | Val Loss 0.8745\n",
      "  Fold 1 | Subgraph 2 | Epoch 020 | Train Loss 0.5238 | Val Loss 0.9229\n",
      "  Early stopping at epoch 26 (best val=0.8593)\n",
      "  Fold 1 | Subgraph 3 | Epoch 001 | Train Loss 1.3679 | Val Loss 1.0108\n",
      "  Fold 1 | Subgraph 3 | Epoch 010 | Train Loss 0.7379 | Val Loss 0.9974\n",
      "  Fold 1 | Subgraph 3 | Epoch 020 | Train Loss 0.5740 | Val Loss 1.0366\n",
      "  Early stopping at epoch 24 (best val=0.9591)\n",
      "  Fold 1 | Subgraph 4 | Epoch 001 | Train Loss 1.2999 | Val Loss 0.8200\n",
      "  Fold 1 | Subgraph 4 | Epoch 010 | Train Loss 0.6580 | Val Loss 0.8356\n",
      "  Fold 1 | Subgraph 4 | Epoch 020 | Train Loss 0.5126 | Val Loss 0.9392\n",
      "  Early stopping at epoch 24 (best val=0.7903)\n",
      "  Fold 1 | Subgraph 5 | Epoch 001 | Train Loss 1.3300 | Val Loss 1.0587\n",
      "  Fold 1 | Subgraph 5 | Epoch 010 | Train Loss 0.6664 | Val Loss 1.0254\n",
      "  Fold 1 | Subgraph 5 | Epoch 020 | Train Loss 0.5555 | Val Loss 1.0488\n",
      "  Fold 1 | Subgraph 5 | Epoch 030 | Train Loss 0.4948 | Val Loss 1.0911\n",
      "  Early stopping at epoch 32 (best val=1.0240)\n",
      "Fold 1: Pearson r = 0.334\n",
      "  Fold 2 | Subgraph 1 | Epoch 001 | Train Loss 1.3506 | Val Loss 1.0390\n",
      "  Fold 2 | Subgraph 1 | Epoch 010 | Train Loss 0.7316 | Val Loss 1.0193\n",
      "  Fold 2 | Subgraph 1 | Epoch 020 | Train Loss 0.5919 | Val Loss 1.0258\n",
      "  Fold 2 | Subgraph 1 | Epoch 030 | Train Loss 0.5243 | Val Loss 1.0091\n",
      "  Fold 2 | Subgraph 1 | Epoch 040 | Train Loss 0.4692 | Val Loss 0.9910\n",
      "  Fold 2 | Subgraph 1 | Epoch 050 | Train Loss 0.4270 | Val Loss 0.9974\n",
      "  Fold 2 | Subgraph 1 | Epoch 060 | Train Loss 0.3801 | Val Loss 1.0144\n",
      "  Early stopping at epoch 64 (best val=0.9818)\n",
      "  Fold 2 | Subgraph 2 | Epoch 001 | Train Loss 1.2240 | Val Loss 0.7988\n",
      "  Fold 2 | Subgraph 2 | Epoch 010 | Train Loss 0.6888 | Val Loss 0.8539\n",
      "  Fold 2 | Subgraph 2 | Epoch 020 | Train Loss 0.5687 | Val Loss 0.9530\n",
      "  Early stopping at epoch 22 (best val=0.7916)\n",
      "  Fold 2 | Subgraph 3 | Epoch 001 | Train Loss 1.2590 | Val Loss 1.0936\n",
      "  Fold 2 | Subgraph 3 | Epoch 010 | Train Loss 0.6345 | Val Loss 1.1247\n",
      "  Fold 2 | Subgraph 3 | Epoch 020 | Train Loss 0.4999 | Val Loss 1.2176\n",
      "  Early stopping at epoch 21 (best val=1.0936)\n",
      "  Fold 2 | Subgraph 4 | Epoch 001 | Train Loss 1.1904 | Val Loss 1.1484\n",
      "  Fold 2 | Subgraph 4 | Epoch 010 | Train Loss 0.6624 | Val Loss 1.0679\n",
      "  Fold 2 | Subgraph 4 | Epoch 020 | Train Loss 0.5318 | Val Loss 1.0430\n",
      "  Fold 2 | Subgraph 4 | Epoch 030 | Train Loss 0.4624 | Val Loss 1.0599\n",
      "  Fold 2 | Subgraph 4 | Epoch 040 | Train Loss 0.4124 | Val Loss 1.0598\n",
      "  Early stopping at epoch 41 (best val=1.0427)\n",
      "  Fold 2 | Subgraph 5 | Epoch 001 | Train Loss 1.2445 | Val Loss 0.9544\n",
      "  Fold 2 | Subgraph 5 | Epoch 010 | Train Loss 0.6357 | Val Loss 0.9370\n",
      "  Fold 2 | Subgraph 5 | Epoch 020 | Train Loss 0.5254 | Val Loss 0.9873\n",
      "  Early stopping at epoch 27 (best val=0.9237)\n",
      "Fold 2: Pearson r = 0.337\n",
      "  Fold 3 | Subgraph 1 | Epoch 001 | Train Loss 1.1970 | Val Loss 1.0659\n",
      "  Fold 3 | Subgraph 1 | Epoch 010 | Train Loss 0.6556 | Val Loss 1.0310\n",
      "  Fold 3 | Subgraph 1 | Epoch 020 | Train Loss 0.5425 | Val Loss 1.0809\n",
      "  Early stopping at epoch 29 (best val=1.0290)\n",
      "  Fold 3 | Subgraph 2 | Epoch 001 | Train Loss 1.3466 | Val Loss 0.9229\n",
      "  Fold 3 | Subgraph 2 | Epoch 010 | Train Loss 0.6842 | Val Loss 0.9161\n",
      "  Fold 3 | Subgraph 2 | Epoch 020 | Train Loss 0.5548 | Val Loss 0.9198\n",
      "  Fold 3 | Subgraph 2 | Epoch 030 | Train Loss 0.4993 | Val Loss 0.9077\n",
      "  Fold 3 | Subgraph 2 | Epoch 040 | Train Loss 0.4470 | Val Loss 0.9512\n",
      "  Early stopping at epoch 46 (best val=0.8917)\n",
      "  Fold 3 | Subgraph 3 | Epoch 001 | Train Loss 1.2402 | Val Loss 1.0077\n",
      "  Fold 3 | Subgraph 3 | Epoch 010 | Train Loss 0.6572 | Val Loss 0.9745\n",
      "  Fold 3 | Subgraph 3 | Epoch 020 | Train Loss 0.5460 | Val Loss 1.0212\n",
      "  Early stopping at epoch 26 (best val=0.9599)\n",
      "  Fold 3 | Subgraph 4 | Epoch 001 | Train Loss 1.2237 | Val Loss 1.0067\n",
      "  Fold 3 | Subgraph 4 | Epoch 010 | Train Loss 0.6530 | Val Loss 1.0003\n",
      "  Fold 3 | Subgraph 4 | Epoch 020 | Train Loss 0.5457 | Val Loss 1.0749\n",
      "  Early stopping at epoch 27 (best val=0.9842)\n",
      "  Fold 3 | Subgraph 5 | Epoch 001 | Train Loss 1.2526 | Val Loss 0.8220\n",
      "  Fold 3 | Subgraph 5 | Epoch 010 | Train Loss 0.6311 | Val Loss 0.8738\n",
      "  Fold 3 | Subgraph 5 | Epoch 020 | Train Loss 0.5452 | Val Loss 0.9885\n",
      "  Early stopping at epoch 22 (best val=0.8141)\n",
      "Fold 3: Pearson r = 0.238\n",
      "  Fold 4 | Subgraph 1 | Epoch 001 | Train Loss 1.2875 | Val Loss 1.0115\n",
      "  Fold 4 | Subgraph 1 | Epoch 010 | Train Loss 0.6561 | Val Loss 0.9659\n",
      "  Fold 4 | Subgraph 1 | Epoch 020 | Train Loss 0.5479 | Val Loss 1.0356\n",
      "  Early stopping at epoch 26 (best val=0.9318)\n",
      "  Fold 4 | Subgraph 2 | Epoch 001 | Train Loss 1.3488 | Val Loss 1.0183\n",
      "  Fold 4 | Subgraph 2 | Epoch 010 | Train Loss 0.6723 | Val Loss 1.0184\n",
      "  Fold 4 | Subgraph 2 | Epoch 020 | Train Loss 0.5567 | Val Loss 1.0828\n",
      "  Early stopping at epoch 24 (best val=0.9732)\n",
      "  Fold 4 | Subgraph 3 | Epoch 001 | Train Loss 1.4098 | Val Loss 1.0674\n",
      "  Fold 4 | Subgraph 3 | Epoch 010 | Train Loss 0.7234 | Val Loss 1.1128\n",
      "  Fold 4 | Subgraph 3 | Epoch 020 | Train Loss 0.6158 | Val Loss 1.1597\n",
      "  Early stopping at epoch 21 (best val=1.0674)\n",
      "  Fold 4 | Subgraph 4 | Epoch 001 | Train Loss 1.2283 | Val Loss 1.0366\n",
      "  Fold 4 | Subgraph 4 | Epoch 010 | Train Loss 0.6429 | Val Loss 1.0082\n",
      "  Fold 4 | Subgraph 4 | Epoch 020 | Train Loss 0.5311 | Val Loss 1.0312\n",
      "  Early stopping at epoch 25 (best val=0.9953)\n",
      "  Fold 4 | Subgraph 5 | Epoch 001 | Train Loss 1.5416 | Val Loss 0.9476\n",
      "  Fold 4 | Subgraph 5 | Epoch 010 | Train Loss 0.7992 | Val Loss 1.0667\n",
      "  Fold 4 | Subgraph 5 | Epoch 020 | Train Loss 0.6328 | Val Loss 1.0853\n",
      "  Early stopping at epoch 22 (best val=0.9432)\n",
      "Fold 4: Pearson r = 0.309\n",
      "  Fold 5 | Subgraph 1 | Epoch 001 | Train Loss 1.1841 | Val Loss 0.9204\n",
      "  Fold 5 | Subgraph 1 | Epoch 010 | Train Loss 0.6071 | Val Loss 0.9680\n",
      "  Fold 5 | Subgraph 1 | Epoch 020 | Train Loss 0.5179 | Val Loss 1.0665\n",
      "  Early stopping at epoch 22 (best val=0.9020)\n",
      "  Fold 5 | Subgraph 2 | Epoch 001 | Train Loss 1.2127 | Val Loss 0.9096\n",
      "  Fold 5 | Subgraph 2 | Epoch 010 | Train Loss 0.6369 | Val Loss 0.9043\n",
      "  Fold 5 | Subgraph 2 | Epoch 020 | Train Loss 0.5266 | Val Loss 0.9782\n",
      "  Early stopping at epoch 25 (best val=0.8826)\n",
      "  Fold 5 | Subgraph 3 | Epoch 001 | Train Loss 1.2051 | Val Loss 1.0695\n",
      "  Fold 5 | Subgraph 3 | Epoch 010 | Train Loss 0.6272 | Val Loss 1.0460\n",
      "  Fold 5 | Subgraph 3 | Epoch 020 | Train Loss 0.5359 | Val Loss 1.1124\n",
      "  Early stopping at epoch 28 (best val=1.0377)\n",
      "  Fold 5 | Subgraph 4 | Epoch 001 | Train Loss 1.2451 | Val Loss 0.9824\n",
      "  Fold 5 | Subgraph 4 | Epoch 010 | Train Loss 0.6989 | Val Loss 0.9722\n",
      "  Fold 5 | Subgraph 4 | Epoch 020 | Train Loss 0.5873 | Val Loss 0.9784\n",
      "  Early stopping at epoch 25 (best val=0.9671)\n",
      "  Fold 5 | Subgraph 5 | Epoch 001 | Train Loss 1.3392 | Val Loss 0.9732\n",
      "  Fold 5 | Subgraph 5 | Epoch 010 | Train Loss 0.7510 | Val Loss 0.9801\n",
      "  Fold 5 | Subgraph 5 | Epoch 020 | Train Loss 0.6392 | Val Loss 0.9709\n",
      "  Fold 5 | Subgraph 5 | Epoch 030 | Train Loss 0.5243 | Val Loss 0.9860\n",
      "  Fold 5 | Subgraph 5 | Epoch 040 | Train Loss 0.4820 | Val Loss 0.9816\n",
      "  Early stopping at epoch 41 (best val=0.9686)\n",
      "Fold 5: Pearson r = 0.277\n",
      "  Fold 6 | Subgraph 1 | Epoch 001 | Train Loss 1.1889 | Val Loss 0.9758\n",
      "  Fold 6 | Subgraph 1 | Epoch 010 | Train Loss 0.6192 | Val Loss 0.9271\n",
      "  Fold 6 | Subgraph 1 | Epoch 020 | Train Loss 0.5216 | Val Loss 0.9527\n",
      "  Early stopping at epoch 26 (best val=0.9190)\n",
      "  Fold 6 | Subgraph 2 | Epoch 001 | Train Loss 1.3784 | Val Loss 1.0122\n",
      "  Fold 6 | Subgraph 2 | Epoch 010 | Train Loss 0.7437 | Val Loss 1.0721\n",
      "  Fold 6 | Subgraph 2 | Epoch 020 | Train Loss 0.5822 | Val Loss 1.1375\n",
      "  Early stopping at epoch 21 (best val=1.0122)\n",
      "  Fold 6 | Subgraph 3 | Epoch 001 | Train Loss 1.1979 | Val Loss 0.9421\n",
      "  Fold 6 | Subgraph 3 | Epoch 010 | Train Loss 0.6601 | Val Loss 0.9307\n",
      "  Fold 6 | Subgraph 3 | Epoch 020 | Train Loss 0.5063 | Val Loss 0.9769\n",
      "  Early stopping at epoch 25 (best val=0.9115)\n",
      "  Fold 6 | Subgraph 4 | Epoch 001 | Train Loss 1.2940 | Val Loss 0.9182\n",
      "  Fold 6 | Subgraph 4 | Epoch 010 | Train Loss 0.6735 | Val Loss 0.9217\n",
      "  Fold 6 | Subgraph 4 | Epoch 020 | Train Loss 0.5572 | Val Loss 0.9734\n",
      "  Early stopping at epoch 26 (best val=0.9022)\n",
      "  Fold 6 | Subgraph 5 | Epoch 001 | Train Loss 1.2192 | Val Loss 0.9180\n",
      "  Fold 6 | Subgraph 5 | Epoch 010 | Train Loss 0.5990 | Val Loss 0.8929\n",
      "  Fold 6 | Subgraph 5 | Epoch 020 | Train Loss 0.5075 | Val Loss 0.9338\n",
      "  Early stopping at epoch 27 (best val=0.8817)\n",
      "Fold 6: Pearson r = 0.232\n",
      "  Fold 7 | Subgraph 1 | Epoch 001 | Train Loss 1.2651 | Val Loss 1.1481\n",
      "  Fold 7 | Subgraph 1 | Epoch 010 | Train Loss 0.6728 | Val Loss 1.1152\n",
      "  Fold 7 | Subgraph 1 | Epoch 020 | Train Loss 0.5596 | Val Loss 1.1728\n",
      "  Early stopping at epoch 26 (best val=1.0986)\n",
      "  Fold 7 | Subgraph 2 | Epoch 001 | Train Loss 1.5289 | Val Loss 0.9258\n",
      "  Fold 7 | Subgraph 2 | Epoch 010 | Train Loss 0.8245 | Val Loss 1.0187\n",
      "  Fold 7 | Subgraph 2 | Epoch 020 | Train Loss 0.6553 | Val Loss 1.0656\n",
      "  Early stopping at epoch 21 (best val=0.9258)\n",
      "  Fold 7 | Subgraph 3 | Epoch 001 | Train Loss 1.4156 | Val Loss 0.9660\n",
      "  Fold 7 | Subgraph 3 | Epoch 010 | Train Loss 0.7425 | Val Loss 0.9509\n",
      "  Fold 7 | Subgraph 3 | Epoch 020 | Train Loss 0.6041 | Val Loss 0.9498\n",
      "  Early stopping at epoch 23 (best val=0.9362)\n",
      "  Fold 7 | Subgraph 4 | Epoch 001 | Train Loss 1.2660 | Val Loss 1.0903\n",
      "  Fold 7 | Subgraph 4 | Epoch 010 | Train Loss 0.6650 | Val Loss 1.0358\n",
      "  Fold 7 | Subgraph 4 | Epoch 020 | Train Loss 0.5537 | Val Loss 1.0044\n",
      "  Fold 7 | Subgraph 4 | Epoch 030 | Train Loss 0.4780 | Val Loss 1.0048\n",
      "  Fold 7 | Subgraph 4 | Epoch 040 | Train Loss 0.4337 | Val Loss 1.0358\n",
      "  Fold 7 | Subgraph 4 | Epoch 050 | Train Loss 0.3801 | Val Loss 1.0430\n",
      "  Early stopping at epoch 53 (best val=0.9989)\n",
      "  Fold 7 | Subgraph 5 | Epoch 001 | Train Loss 1.2513 | Val Loss 0.9850\n",
      "  Fold 7 | Subgraph 5 | Epoch 010 | Train Loss 0.6606 | Val Loss 0.9037\n",
      "  Fold 7 | Subgraph 5 | Epoch 020 | Train Loss 0.5340 | Val Loss 0.9149\n",
      "  Fold 7 | Subgraph 5 | Epoch 030 | Train Loss 0.4752 | Val Loss 0.9138\n",
      "  Early stopping at epoch 30 (best val=0.9037)\n",
      "Fold 7: Pearson r = 0.240\n",
      "  Fold 8 | Subgraph 1 | Epoch 001 | Train Loss 1.4082 | Val Loss 0.8366\n",
      "  Fold 8 | Subgraph 1 | Epoch 010 | Train Loss 0.7523 | Val Loss 0.9281\n",
      "  Fold 8 | Subgraph 1 | Epoch 020 | Train Loss 0.6109 | Val Loss 1.0074\n",
      "  Early stopping at epoch 22 (best val=0.8255)\n",
      "  Fold 8 | Subgraph 2 | Epoch 001 | Train Loss 1.2504 | Val Loss 1.1078\n",
      "  Fold 8 | Subgraph 2 | Epoch 010 | Train Loss 0.6927 | Val Loss 1.0782\n",
      "  Fold 8 | Subgraph 2 | Epoch 020 | Train Loss 0.5889 | Val Loss 1.1064\n",
      "  Early stopping at epoch 25 (best val=1.0766)\n",
      "  Fold 8 | Subgraph 3 | Epoch 001 | Train Loss 1.2350 | Val Loss 1.0916\n",
      "  Fold 8 | Subgraph 3 | Epoch 010 | Train Loss 0.6512 | Val Loss 1.0736\n",
      "  Fold 8 | Subgraph 3 | Epoch 020 | Train Loss 0.5372 | Val Loss 1.1260\n",
      "  Early stopping at epoch 25 (best val=1.0649)\n",
      "  Fold 8 | Subgraph 4 | Epoch 001 | Train Loss 1.2646 | Val Loss 0.9140\n",
      "  Fold 8 | Subgraph 4 | Epoch 010 | Train Loss 0.6702 | Val Loss 0.8618\n",
      "  Fold 8 | Subgraph 4 | Epoch 020 | Train Loss 0.5481 | Val Loss 0.9140\n",
      "  Early stopping at epoch 26 (best val=0.8465)\n",
      "  Fold 8 | Subgraph 5 | Epoch 001 | Train Loss 1.3200 | Val Loss 0.9314\n",
      "  Fold 8 | Subgraph 5 | Epoch 010 | Train Loss 0.7187 | Val Loss 0.9476\n",
      "  Fold 8 | Subgraph 5 | Epoch 020 | Train Loss 0.5715 | Val Loss 0.9750\n",
      "  Early stopping at epoch 25 (best val=0.9007)\n",
      "Fold 8: Pearson r = 0.201\n",
      "  Fold 9 | Subgraph 1 | Epoch 001 | Train Loss 1.2362 | Val Loss 0.7665\n",
      "  Fold 9 | Subgraph 1 | Epoch 010 | Train Loss 0.6490 | Val Loss 0.7506\n",
      "  Fold 9 | Subgraph 1 | Epoch 020 | Train Loss 0.5470 | Val Loss 0.8152\n",
      "  Early stopping at epoch 25 (best val=0.7175)\n",
      "  Fold 9 | Subgraph 2 | Epoch 001 | Train Loss 1.2328 | Val Loss 1.0315\n",
      "  Fold 9 | Subgraph 2 | Epoch 010 | Train Loss 0.6852 | Val Loss 0.9184\n",
      "  Fold 9 | Subgraph 2 | Epoch 020 | Train Loss 0.5496 | Val Loss 0.9639\n",
      "  Fold 9 | Subgraph 2 | Epoch 030 | Train Loss 0.4776 | Val Loss 0.9905\n",
      "  Early stopping at epoch 31 (best val=0.9175)\n",
      "  Fold 9 | Subgraph 3 | Epoch 001 | Train Loss 1.2822 | Val Loss 0.8650\n",
      "  Fold 9 | Subgraph 3 | Epoch 010 | Train Loss 0.7032 | Val Loss 0.7997\n",
      "  Fold 9 | Subgraph 3 | Epoch 020 | Train Loss 0.5460 | Val Loss 0.8064\n",
      "  Early stopping at epoch 29 (best val=0.7992)\n",
      "  Fold 9 | Subgraph 4 | Epoch 001 | Train Loss 1.2831 | Val Loss 0.9794\n",
      "  Fold 9 | Subgraph 4 | Epoch 010 | Train Loss 0.6940 | Val Loss 0.9283\n",
      "  Fold 9 | Subgraph 4 | Epoch 020 | Train Loss 0.5631 | Val Loss 0.9353\n",
      "  Early stopping at epoch 26 (best val=0.9191)\n",
      "  Fold 9 | Subgraph 5 | Epoch 001 | Train Loss 1.1978 | Val Loss 1.2214\n",
      "  Fold 9 | Subgraph 5 | Epoch 010 | Train Loss 0.6505 | Val Loss 1.2611\n",
      "  Fold 9 | Subgraph 5 | Epoch 020 | Train Loss 0.5245 | Val Loss 1.2942\n",
      "  Early stopping at epoch 23 (best val=1.1955)\n",
      "Fold 9: Pearson r = 0.266\n",
      "  Fold 10 | Subgraph 1 | Epoch 001 | Train Loss 1.3510 | Val Loss 0.9374\n",
      "  Fold 10 | Subgraph 1 | Epoch 010 | Train Loss 0.7248 | Val Loss 0.8932\n",
      "  Fold 10 | Subgraph 1 | Epoch 020 | Train Loss 0.6069 | Val Loss 0.9770\n",
      "  Early stopping at epoch 27 (best val=0.8813)\n",
      "  Fold 10 | Subgraph 2 | Epoch 001 | Train Loss 1.2905 | Val Loss 0.9363\n",
      "  Fold 10 | Subgraph 2 | Epoch 010 | Train Loss 0.6510 | Val Loss 0.9052\n",
      "  Fold 10 | Subgraph 2 | Epoch 020 | Train Loss 0.5560 | Val Loss 0.9680\n",
      "  Early stopping at epoch 27 (best val=0.8933)\n",
      "  Fold 10 | Subgraph 3 | Epoch 001 | Train Loss 1.2166 | Val Loss 0.9250\n",
      "  Fold 10 | Subgraph 3 | Epoch 010 | Train Loss 0.6535 | Val Loss 0.9303\n",
      "  Fold 10 | Subgraph 3 | Epoch 020 | Train Loss 0.5507 | Val Loss 0.9963\n",
      "  Early stopping at epoch 24 (best val=0.9013)\n",
      "  Fold 10 | Subgraph 4 | Epoch 001 | Train Loss 1.5125 | Val Loss 0.9658\n",
      "  Fold 10 | Subgraph 4 | Epoch 010 | Train Loss 0.8123 | Val Loss 1.0210\n",
      "  Fold 10 | Subgraph 4 | Epoch 020 | Train Loss 0.6534 | Val Loss 0.9581\n",
      "  Early stopping at epoch 22 (best val=0.9447)\n",
      "  Fold 10 | Subgraph 5 | Epoch 001 | Train Loss 1.2310 | Val Loss 1.2254\n",
      "  Fold 10 | Subgraph 5 | Epoch 010 | Train Loss 0.6856 | Val Loss 1.1220\n",
      "  Fold 10 | Subgraph 5 | Epoch 020 | Train Loss 0.5705 | Val Loss 1.1211\n",
      "  Fold 10 | Subgraph 5 | Epoch 030 | Train Loss 0.4787 | Val Loss 1.1538\n",
      "  Early stopping at epoch 36 (best val=1.1108)\n",
      "Fold 10: Pearson r = 0.264\n",
      "Overall Pearson r = 0.264\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# ---------- device ----------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "N = X_features.shape[0]\n",
    "assert y_values.shape[0] == N\n",
    "# ---------- CV + RS training ----------\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "all_test_preds = []\n",
    "all_test_true  = []\n",
    "fold_num = 0\n",
    "\n",
    "for train_index, test_index in kf.split(np.arange(N)):\n",
    "    fold_num += 1\n",
    "    train_index = np.array(train_index)\n",
    "    test_index  = np.array(test_index)\n",
    "\n",
    "    # --- PCA transform (fit only on train) ---\n",
    "    if PCA_DIM:\n",
    "        pca = PCA(n_components=PCA_DIM, svd_solver=\"randomized\")\n",
    "        X_train_np = X_features[train_index].cpu().numpy()\n",
    "        X_test_np  = X_features[test_index].cpu().numpy()\n",
    "        pca.fit(X_train_np)\n",
    "        X_train_pca = torch.tensor(pca.transform(X_train_np), dtype=torch.float32)\n",
    "        X_test_pca  = torch.tensor(pca.transform(X_test_np), dtype=torch.float32)\n",
    "        # rebuild full tensor for easy indexing\n",
    "        X_fold = torch.zeros((N, PCA_DIM), dtype=torch.float32)\n",
    "        X_fold[train_index] = X_train_pca\n",
    "        X_fold[test_index]  = X_test_pca\n",
    "    else:\n",
    "        X_fold = X_features\n",
    "\n",
    "    # --- Standardize features and y ---\n",
    "    X_std, X_mu, X_sd = standardize_train_test(X_fold, train_index, test_index)\n",
    "    y_std, y_mu, y_sd = standardize_y_train_test(y_values, train_index, test_index)\n",
    "\n",
    "    # Move tensors to device\n",
    "    X_std = X_std.to(device)\n",
    "    y_std = y_std.to(device)\n",
    "\n",
    "    # Induced test graph (row-norm) once\n",
    "    A_test = subgraph_norm_to_sparse(A_knn_csr, test_index).to(device)\n",
    "    \n",
    "    preds_ensemble = torch.zeros(len(test_index), device=device)\n",
    "\n",
    "    for s in range(subgraph_count):\n",
    "        sub_size = int(subgraph_frac * len(train_index))\n",
    "        sub_nodes = np.random.choice(train_index, size=sub_size, replace=False)\n",
    "        A_sub = subgraph_norm_to_sparse(A_knn_csr, sub_nodes).to(device)\n",
    "\n",
    "        rng = np.random.default_rng(1234 + s)\n",
    "        perm = rng.permutation(len(sub_nodes))\n",
    "        val_count = max(1, int(val_frac * len(sub_nodes)))\n",
    "        val_local = perm[:val_count]\n",
    "        tr_local  = perm[val_count:]\n",
    "\n",
    "        sub_nodes_t = torch.tensor(sub_nodes, dtype=torch.long, device=device)\n",
    "        tr_idx = sub_nodes_t[tr_local]\n",
    "        va_idx = sub_nodes_t[val_local]\n",
    "\n",
    "        X_tr = X_std[tr_idx]\n",
    "        y_tr = y_std[tr_idx]\n",
    "        X_va = X_std[va_idx]\n",
    "        y_va = y_std[va_idx]\n",
    "\n",
    "        A_sub_coo = A_sub.coalesce()\n",
    "        A_sub_np  = sp.coo_matrix((A_sub_coo.values().cpu().numpy(),\n",
    "                                   (A_sub_coo.indices()[0].cpu().numpy(),\n",
    "                                    A_sub_coo.indices()[1].cpu().numpy())),\n",
    "                                  shape=A_sub.shape).tocsr()\n",
    "        A_tr = subgraph_norm_to_sparse(A_sub_np, tr_local).to(device)\n",
    "        A_va = subgraph_norm_to_sparse(A_sub_np, val_local).to(device)\n",
    "\n",
    "        # init model (input_dim depends on PCA)\n",
    "        model = GCN_RS_Model(\n",
    "            input_dim=X_std.shape[1],   ### use post-PCA dim\n",
    "            hidden_dims=HIDDEN_DIMS,\n",
    "            dropout=DROPOUT,\n",
    "            use_batchnorm=USE_BATCHNORM\n",
    "        ).to(device)\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        loss_fn = nn.MSELoss()\n",
    "\n",
    "        best_state = None\n",
    "        best_val   = float(\"inf\")\n",
    "        patience_ct = 0\n",
    "\n",
    "        for epoch in range(1, max_epochs + 1):\n",
    "            model.train()\n",
    "            opt.zero_grad()\n",
    "            yhat_tr = model(X_tr, A_tr)\n",
    "            loss_tr = loss_fn(yhat_tr, y_tr)\n",
    "            loss_tr.backward()\n",
    "            opt.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                yhat_va = model(X_va, A_va)\n",
    "                loss_va = loss_fn(yhat_va, y_va).item()\n",
    "\n",
    "            if epoch % 10 == 0 or epoch == max_epochs or epoch == 1:\n",
    "                print(f\"  Fold {fold_num} | Subgraph {s+1} | Epoch {epoch:03d} \"\n",
    "                      f\"| Train Loss {loss_tr.item():.4f} | Val Loss {loss_va:.4f}\")\n",
    "\n",
    "            if loss_va < best_val - 1e-6:\n",
    "                best_val = loss_va\n",
    "                best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "                patience_ct = 0\n",
    "            else:\n",
    "                patience_ct += 1\n",
    "                if patience_ct >= patience:\n",
    "                    print(f\"  Early stopping at epoch {epoch} (best val={best_val:.4f})\")\n",
    "                    break\n",
    "\n",
    "        if best_state is not None:\n",
    "            model.load_state_dict(best_state)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_te = X_std[torch.tensor(test_index, dtype=torch.long, device=device)]\n",
    "            yhat_te_std = model(X_te, A_test)\n",
    "            preds_ensemble += yhat_te_std\n",
    "\n",
    "    preds_ensemble /= subgraph_count\n",
    "\n",
    "    yhat_te = preds_ensemble * y_sd + y_mu\n",
    "    y_te    = y_values[torch.tensor(test_index, dtype=torch.long, device=device)]\n",
    "\n",
    "    all_test_preds.extend(yhat_te.detach().cpu().numpy().tolist())\n",
    "    all_test_true.extend(y_te.detach().cpu().numpy().tolist())\n",
    "\n",
    "    r = np.corrcoef(yhat_te.detach().cpu().numpy(), y_te.detach().cpu().numpy())[0, 1]\n",
    "    print(f\"Fold {fold_num}: Pearson r = {r:.3f}\")\n",
    "\n",
    "all_test_preds = np.asarray(all_test_preds)\n",
    "all_test_true  = np.asarray(all_test_true)\n",
    "overall_r = np.corrcoef(all_test_preds, all_test_true)[0, 1]\n",
    "print(f\"Overall Pearson r = {overall_r:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3de07126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simen\\AppData\\Local\\Temp\\ipykernel_13908\\1047844523.py:17: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.boxplot(x=dataset, y=pearson_r, palette=[\"skyblue\", \"lightcoral\"])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "pearson_r_all = np.array([0.339, 0.326, 0.280, 0.233, 0.362, 0.300, 0.305, 0.386, 0.356, 0.260])\n",
    "pearson_r_helgeland = np.array([0.334, 0.337, 0.238, 0.309, 0.277, 0.232, 0.240, 0.201, 0.266, 0.264])\n",
    "\n",
    "# Combine into dataset\n",
    "pearson_r = np.concatenate([pearson_r_all, pearson_r_helgeland])\n",
    "dataset = [\"All\"] * len(pearson_r_all) + [\"Helgeland\"] * len(pearson_r_helgeland)\n",
    "\n",
    "# Plot\n",
    "sns.set(style=\"whitegrid\", palette=\"pastel\")\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "sns.boxplot(x=dataset, y=pearson_r, palette=[\"skyblue\", \"lightcoral\"])\n",
    "sns.stripplot(x=dataset, y=pearson_r, color=\"black\", size=6, jitter=True)\n",
    "\n",
    "# Add mean markers\n",
    "means = [pearson_r_all.mean(), pearson_r_helgeland.mean()]\n",
    "for i, mean in enumerate(means):\n",
    "    plt.scatter(i, mean, color=\"red\", marker=\"D\", s=60, zorder=10, label=\"Mean\" if i == 0 else \"\")\n",
    "\n",
    "plt.title(\"Comparison of Pearson Correlations\\nAll vs Helgeland (10% validation)\", fontsize=14)\n",
    "plt.ylabel(\"Pearson r\", fontsize=12)\n",
    "plt.xlabel(\"Dataset\", fontsize=12)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
